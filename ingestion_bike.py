import pandas as pd
import requests
from io import StringIO
import os
import boto3
from datetime import datetime, timezone



DATA_URL = "https://data.rennesmetropole.fr/explore/dataset/eco-counter-data/download/?format=csv&timezone=Europe/Paris&use_labels_for_header=true"

S3_BUCKET_NAME = "cityflow-raw0"

S3_PREFIX = "bike/"

LOCAL_REFERENCE_FILE = "cleaned_data.csv"

START_DATE = datetime(2025, 9, 1, tzinfo=timezone.utc)




def download_data(url: str):
    """T√©l√©charge les donn√©es depuis l'URL source"""
    print("üöÄ T√©l√©chargement des donn√©es...")
    response = requests.get(url)
    if response.status_code == 200:
        print("‚úÖ T√©l√©chargement r√©ussi.")
        return response.content.decode("utf-8")
    print(f"‚ùå Erreur de t√©l√©chargement : {response.status_code}")
    return None


def load_data(data: str):
    """Charge le CSV en DataFrame pandas"""
    df = pd.read_csv(StringIO(data), delimiter=";")
    print(f"üì• Donn√©es charg√©es : {len(df)} lignes.")
    print("Colonnes d√©tect√©es :", df.columns.tolist())
    return df


def clean_data(df: pd.DataFrame):
    """Nettoie et formate les donn√©es brutes"""
    print("üßπ Nettoyage des donn√©es...")

    df.rename(columns={
        "date": "Date",
        "isodate": "ISO_Date",
        "counts": "Counts",
        "status": "Status",
        "id": "Sensor_ID",
        "name": "Location_Name",
        "geo": "Coordinates",
        "sens": "Direction"
    }, inplace=True, errors="ignore")

    # Conversion des dates et valeurs
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce", utc=True)
    df["Counts"] = pd.to_numeric(df["Counts"], errors="coerce")
    df.dropna(subset=["Date", "Counts"], inplace=True)

    # Filtrer √† partir de la date d√©finie
    df = df[df["Date"] >= START_DATE]

    print(f"‚úÖ {len(df)} lignes apr√®s nettoyage (depuis {START_DATE.date()})")
    return df



def get_latest_date_from_s3(bucket_name, prefix="bike/"):
    """R√©cup√®re la derni√®re date de donn√©es pr√©sente sur S3"""
    s3 = boto3.client("s3")
    try:
        objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
        if "Contents" not in objects:
            print("üì≠ Aucun fichier trouv√© dans S3.")
            return None

        latest_file = max(objects["Contents"], key=lambda x: x["LastModified"])["Key"]
        print(f"üì¶ Dernier fichier S3 d√©tect√© : {latest_file}")

        csv_obj = s3.get_object(Bucket=bucket_name, Key=latest_file)
        df_last = pd.read_csv(csv_obj["Body"])
        if "Date" in df_last.columns:
            last_date = pd.to_datetime(df_last["Date"], utc=True).max()
            print(f"üïì Derni√®re date trouv√©e dans S3 : {last_date}")
            return last_date
        return None
    except Exception as e:
        print("‚ö†Ô∏è Erreur S3 :", e)
        return None


def upload_to_s3(df: pd.DataFrame, bucket_name: str, file_name: str):
    """Charge un fichier CSV sur S3"""
    s3 = boto3.client("s3")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    s3.put_object(Bucket=bucket_name, Key=file_name, Body=csv_buffer.getvalue())
    print(f"‚úÖ Fichier envoy√© : s3://{bucket_name}/{file_name}")



def main():
    print("============== üåÜ D√âBUT DU BATCH CITYFLOW ==============")

    # √âtape 1 ‚Äî T√©l√©charger les donn√©es
    data = download_data(DATA_URL)
    if not data:
        print("‚ùå T√©l√©chargement √©chou√©, arr√™t du batch.")
        return

    # √âtape 2 ‚Äî Charger et nettoyer
    df = load_data(data)
    df_cleaned = clean_data(df)

    # √âtape 3 ‚Äî D√©terminer la derni√®re date
    latest_date = None
    if os.path.exists(LOCAL_REFERENCE_FILE):
        existing = pd.read_csv(LOCAL_REFERENCE_FILE)
        latest_date = pd.to_datetime(existing["Date"], utc=True).max()
        print(f"üïì Derni√®re date locale connue : {latest_date}")
    else:
        latest_date = get_latest_date_from_s3(S3_BUCKET_NAME, prefix=S3_PREFIX)

    # √âtape 4 ‚Äî Filtrer les nouvelles donn√©es
    if latest_date is not None:
        new_data = df_cleaned[df_cleaned["Date"] > latest_date]
    else:
        new_data = df_cleaned

    if new_data.empty:
        print("‚ÑπÔ∏è Aucune nouvelle donn√©e √† charger.")
        print("============== ‚úÖ FIN DU BATCH (aucune mise √† jour) ==============")
        return

    # √âtape 5 ‚Äî Mettre √† jour le fichier local
    if os.path.exists(LOCAL_REFERENCE_FILE):
        combined = pd.concat([existing, new_data]).drop_duplicates(subset=["Date", "Sensor_ID"])
    else:
        combined = new_data
    combined.to_csv(LOCAL_REFERENCE_FILE, index=False)
    print(f"üíæ Fichier local mis √† jour : {LOCAL_REFERENCE_FILE}")

    # √âtape 6 ‚Äî Envoi sur S3
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M")
    s3_key = f"{S3_PREFIX}cleaned_data_delta_{timestamp}.csv"
    upload_to_s3(new_data, S3_BUCKET_NAME, s3_key)

    print(f"üìà {len(new_data)} nouvelles lignes envoy√©es.")
    print("============== ‚úÖ FIN DU BATCH CITYFLOW ==============")



if __name__ == "__main__":
    main()
